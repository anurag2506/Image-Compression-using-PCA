# Image-Compression-using-PCA
<h5>This project dives into Principal Component Analysis and how it can be used for reducing dimensions of complex features and choose specific dimensions 
  that maximize the variance </h5>
<h4> WHAT IS PCA?</h4>
<p>Imagine you have a large dataset with many variables. Visualizing and analyzing such data can be cumbersome. PCA comes in to **reduce the number of variables** while capturing most of the important information. ***It does this by creating a new set of variables, called principal components (PCs), that are linear combinations of the original ones.***

Basically what PCA does it reduces the dimensionality of any dataset by projecting the points on an arbitrary line and this line preserves the most variance. 

Why do we choose variance as the deciding factor?

PCA understands which part of our data is important only via calculating variance

- Variance can mathematically quantify the amount of information embedded within the data
- The greater the variance, the more the information. Vice versa.
  
  ![image](https://github.com/anurag2506/Image-Compression-using-PCA/assets/142666721/4d8bd104-c667-4843-a524-9f9d81488bd8)
  
How does variance associate with information?

Eg. 

Suppose that we are playing a guessing game with our friends. The game is simple. Our friends would cover their faces and we need to guess whoâ€™s who based solely on their height. Being the good friends that we are, we remember how tall everyone is.</p>
![image](https://github.com/anurag2506/Image-Compression-using-PCA/assets/142666721/50131b47-a186-4700-89ff-f60809beb1b7)
![image](https://github.com/anurag2506/Image-Compression-using-PCA/assets/142666721/253d4e07-16af-41f2-b5d8-ed2c9e3a3dd8)

The difference between the 2 tables is the value of variance. The variance being high in the first table increases the chances of randomly guessing and getting the correct person. But as the variance decreases as in the 2nd table, our chances go down.

<h4>
  **Key Concepts:**
</h4>
<p>
1. **Dimensionality Reduction:** PCA aims to find a lower-dimensional space that captures the maximum variance in the data. Think of it as finding the most informative directions (axes) in your data and projecting the data points onto those directions.
2. **Principal Components (PCs):** These are the new variables created by PCA. They are uncorrelated (meaning no redundancy) and ordered by their importance (variance captured). The first PC captures the most variance, the second captures the remaining most variance, and so on.
3. **Eigenvectors and Eigenvalues:** PCA relies on linear algebra concepts like eigenvectors and eigenvalues. Eigenvectors represent the directions (axes) of the PCs, and eigenvalues represent the amount of variance captured by each PC.

Many machine learning algorithms struggle with high-dimensional data. PCA can be used as a preprocessing step to reduce dimensionality and improve the performance of these algorithms.
Since PCAs form these new variables which are linear combinations of existing variables, for ML features having strong non-linear relations, PCA might not be the best choice
</p>

<h3>Dependencies:</h3>
<p>
1. sklearn
2. sci-kit learn
3. matplotlib
4. OpenCV
</p>

